{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "import albumentations as abm\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \"\"\"depth estimation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, csv_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory containing image folders.\n",
    "            keyword: examples are \"img_train\"\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_dir = csv_dir\n",
    "        self.labels = pd.read_csv(self.csv_dir, header = None)\n",
    "        print(\"self.labels\", self.labels)\n",
    "        self.transform = transform\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get label\n",
    "        label = np.reshape(self.labels.iloc[idx,:].values, (2, 2))\n",
    "        label = label.astype('float')\n",
    "        \n",
    "        # get relevant frame\n",
    "        img_path = os.path.join(self.img_dir, \"camera_image_{:06d}\".format(idx) + \".jpg\")\n",
    "        image = io.imread(img_path)\n",
    "        sample = {'image': image, 'label':label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation - Must be applied in the order as follows\n",
    "class ShiftScaleRotate(object):\n",
    "    def __init__(self, p = 0.8):\n",
    "        self.p = p\n",
    "        self.ssr = abm.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.3, rotate_limit=20,\n",
    "                                    interpolation=1, border_mode=4,\n",
    "                                    value=None, mask_value=None,\n",
    "                                    p=self.p) # always_apply=True\n",
    "        self.composed_functions = abm.Compose([self.ssr], keypoint_params=abm.KeypointParams(format='xy', remove_invisible=True))\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['label']\n",
    "        transformed = self.composed_functions(image = image, keypoints = label)\n",
    "        image = transformed['image']\n",
    "        label = transformed['keypoints']\n",
    "\n",
    "        return{'image':image,\n",
    "               'label':label\n",
    "        }\n",
    "\n",
    "class RandomSizedBBoxSafeCrop(object):\n",
    "    \"\"\"Crop such that the keypoints will be in the bounding box\n",
    "    To make sure that the tool-tip will be visible, add an extra keypoint\n",
    "    to the right of the needle (see code below)\n",
    "    Note: keypoints are not implemented, so use tool-tip as a bounding box param\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.8):\n",
    "        self.p = p\n",
    "        \n",
    "        self.category_ids = [0, 1, 2, 3]\n",
    "        self.ssr = abm.RandomSizedBBoxSafeCrop(width=512, height=512, erosion_rate=0.1, interpolation=1, \n",
    "                                               always_apply = True)\n",
    "        self.centerCrop = abm.CenterCrop (height=512, width =512, always_apply=True)\n",
    "        \n",
    "        self.composed_functions = abm.Compose([self.ssr], \n",
    "                                bbox_params=abm.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "        # apply center crop if not performing above aug\n",
    "        self.composed_functions_alt = abm.Compose([self.centerCrop], \n",
    "                                              keypoint_params=abm.KeypointParams(format='xy'))\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "    \n",
    "        # perform random sized bbox safe crop\n",
    "        if (np.random.uniform() < self.p):\n",
    "            transformed = self.composed_functions(image = image, \n",
    "                                bboxes = [[label[0][0], label[0][1], 1, 1], \n",
    "                                          [label[1][0], label[1][1], 1, 1],\n",
    "                                          [label[0][0], label[0][1], 4, 4], \n",
    "                                          [label[1][0], label[1][1], 4, 4]],\n",
    "                                category_ids = self.category_ids)\n",
    "            image = transformed['image']\n",
    "            label = np.array((transformed['bboxes'][0][0:2], transformed['bboxes'][1][0:2])).reshape(2, 2)\n",
    "        # otherwise just center crop\n",
    "        else:\n",
    "            transformed = self.composed_functions_alt(image = image, keypoints = label)\n",
    "            image = transformed['image']\n",
    "            label = transformed['keypoints']\n",
    "            \n",
    "        return{'image':image,\n",
    "               'label':label,\n",
    "        }\n",
    "    \n",
    "class SafeRotate(object):  \n",
    "    def __init__(self, p = 1):\n",
    "        self.p = p\n",
    "        self.ssr = abm.SafeRotate(limit=[90, 90], interpolation=1, \n",
    "                                   border_mode=4, value=None, \n",
    "                                   mask_value=None, always_apply=True, p=self.p)\n",
    "        self.composed_functions = abm.Compose([self.ssr], keypoint_params=abm.KeypointParams(format='xy'))      \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['label']\n",
    "        transformed = self.composed_functions(image = image, keypoints = label)\n",
    "        image = transformed['image']\n",
    "        label = transformed['keypoints']\n",
    "\n",
    "        return{'image':image,\n",
    "               'label':label,\n",
    "        }\n",
    "\n",
    "class Crop(object):\n",
    "    def __init__(self, x_min=0, y_min=0, x_max=700, y_max=500, always_apply = False, p = 1.0):\n",
    "        self.p = p\n",
    "        self.x_min = x_min\n",
    "        self.y_min = y_min\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "        self.CenterCrop = abm.Crop(x_min= self.x_min, y_min= self.y_min, x_max= self.x_max, y_max= self.y_max, p = self.p)\n",
    "        self.composed_functions = abm.Compose([self.CenterCrop], keypoint_params=abm.KeypointParams(format='xy'))\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        transformed = self.composed_functions(image = image, keypoints = label)\n",
    "        image = transformed['image']\n",
    "        label = transformed['keypoints']\n",
    "        \n",
    "        return{'image':image,\n",
    "               'label':label\n",
    "        }\n",
    "    \n",
    "class CenterCrop(object):\n",
    "    def __init__(self, height = 480, width = 640, always_apply = False, p = 1.0):\n",
    "        self.p = p\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.CenterCrop = abm.CenterCrop(height = self.height, width = self.width, p = self.p)\n",
    "        self.composed_functions = abm.Compose([self.CenterCrop], keypoint_params=abm.KeypointParams(format='xy'))\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        transformed = self.composed_functions(image = image, keypoints = label)\n",
    "        image = transformed['image']\n",
    "        label = transformed['keypoints']\n",
    "        \n",
    "        return{'image':image,\n",
    "               'label':label\n",
    "        }\n",
    "\n",
    "class Blur(object):  \n",
    "    def __init__(self, p = 0.8):\n",
    "        self.p = p\n",
    "        self.composed_functions = abm.Compose(\n",
    "            [abm.Blur(p=self.p)], \n",
    "            keypoint_params=abm.KeypointParams(format='xy')\n",
    "        )\n",
    "        \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "    \n",
    "        transformed = self.composed_functions(image = image, keypoints = label)\n",
    "        image = transformed['image']\n",
    "        label = transformed['keypoints']\n",
    "        \n",
    "        return{'image':image,\n",
    "               'label':label,\n",
    "        }\n",
    "\n",
    "class Jitter(object):\n",
    "    def __init__(self, p = 0.8):\n",
    "        self.p = p\n",
    "        self.jitter = torchvision.transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25)\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        # jitter 20 percent of the time                                            \n",
    "        if (np.random.uniform() < self.p):\n",
    "            image = TF.to_pil_image(image)\n",
    "            image = self.jitter(image)\n",
    "\n",
    "            image = np.array(image)\n",
    "        \n",
    "        \n",
    "        return{'image':image,\n",
    "               'label':label\n",
    "        }\n",
    "\n",
    "    \n",
    "class AddHueSaturationRGBShift(object):  \n",
    "    def __init__(self, p = 0.8):\n",
    "        self.p = p\n",
    "        self.hue_saturation = abm.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20)\n",
    "        self.rgb_shift = abm.RGBShift (r_shift_limit=20, g_shift_limit=20, b_shift_limit=20)\n",
    "        self.composed_functions = abm.Compose([self.hue_saturation, self.rgb_shift])\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        # add noise and dropout 20 percent of the time                 \n",
    "        if (np.random.uniform() < self.p):\n",
    "            image = {'image': image}\n",
    "            image = self.composed_functions(**image)\n",
    "            image = image['image']\n",
    "        \n",
    "        \n",
    "        return{'image':image,\n",
    "               'label':label\n",
    "        }\n",
    "    \n",
    "class AddNoiseAddDropout(object):  \n",
    "    def __init__(self, p = 0.8):\n",
    "        self.p = p\n",
    "        self.dropout_function = abm.CoarseDropout(max_holes = 140, max_height = 5, \n",
    "                                                  max_width = 5, always_apply = True)\n",
    "        self.noise_function = abm.GaussNoise(always_apply=True)\n",
    "        self.composed_functions = abm.Compose([self.dropout_function, self.noise_function])\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        # add noise and dropout 20 percent of the time                 \n",
    "        if (np.random.uniform() < self.p):\n",
    "            image = {'image': image}\n",
    "            image = self.composed_functions(**image)\n",
    "            image = image['image']\n",
    "        \n",
    "        \n",
    "        return{'image':image,\n",
    "               'label':label\n",
    "        }\n",
    "\n",
    "class AddTargetLayer(object):\n",
    "    \"\"\"Add a binary target layer to each image.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sz = 20\n",
    "        image, label = [sample['image'], sample['label']]\n",
    "        target_image = np.zeros((512, 512))\n",
    "        target_image[int(label[1][1])-sz:int(label[1][1])+sz+1, int(label[1][0])-sz:int(label[1][0])+sz+1] = 1 \n",
    "        target_image = target_image[:,:,np.newaxis]*255\n",
    "\n",
    "        # combine inputs\n",
    "        image = np.concatenate((image, target_image), axis = 2) / 255.0\n",
    "        \n",
    "        return{'image':image, 'label': label} \n",
    "\n",
    "class ConvertToolTipPositionToImageLabel(object):\n",
    "    \"\"\"Convert point labels to class labelss (0 - 7).\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = [sample['image'], sample['label']]\n",
    "        # create label\n",
    "        label = np.array(label)\n",
    "        label = label.astype('float')\n",
    "        v = label[1] - label[0]\n",
    "        angle = np.arctan2(-v[1], v[0]) * 180 / np.pi\n",
    "        if angle >= 157.5 and angle <= 180 or angle < -157.5:\n",
    "            cur_class_label = 0\n",
    "        else:\n",
    "            cur_class_label = int((angle + 202.5) / 45)\n",
    "        if (np.linalg.norm(v) < 2.5):\n",
    "            cur_class_label = 8\n",
    "        \n",
    "        return{'image':image, 'label': label, 'class_label': cur_class_label} \n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label, class_label = [sample['image'], sample['label'], sample['class_label']]\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = TF.to_tensor(image)\n",
    "#         image = image.unsqueeze(0)\n",
    "        class_label = torch.tensor(class_label)\n",
    "        \n",
    "        return{'image':image, 'label': label, 'class_label': class_label} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Test Dataset\n",
    "datasets = []\n",
    "train_datasets = []\n",
    "validation_datasets = []\n",
    "# train dataset\n",
    "data_transform_train = transforms.Compose([\n",
    "            ShiftScaleRotate(),\n",
    "            RandomSizedBBoxSafeCrop(),\n",
    "            Blur(),\n",
    "            Jitter(),\n",
    "            AddHueSaturationRGBShift(),\n",
    "            AddNoiseAddDropout(),\n",
    "            AddTargetLayer(),\n",
    "            ConvertToolTipPositionToImageLabel(),\n",
    "            ToTensor()\n",
    "            ])\n",
    "\n",
    "data_transform_validation = transforms.Compose([\n",
    "            AddTargetLayer(),\n",
    "            ConvertToolTipPositionToImageLabel(),\n",
    "            ToTensor()\n",
    "            ])\n",
    "train_dataset = Dataset(img_dir = \"../dataset_for_all/navigation_network/training_data\", csv_dir = \"training_tool_tip_and_targets_label.csv\",  \n",
    "                        transform = data_transform_train) #= data_transform)\n",
    "\n",
    "# test dataset\n",
    "validation_dataset = Dataset(img_dir = \"../dataset_for_all/navigation_network/validation_data\", csv_dir = \"validation_tool_tip_and_targets_label.csv\",  \n",
    "                       transform = data_transform_validation) #= data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training_dataset total images:\", len(train_dataset))\n",
    "print(\"validation_dataset total images:\", len(validation_dataset))\n",
    "print(\"training percentage: \", len(train_dataset)/(len(train_dataset) + len(validation_dataset)))\n",
    "print(\"validation percentage: \", len(validation_dataset)/(len(train_dataset) + len(validation_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(16200, 16203):\n",
    "    print(\"index\", i)\n",
    "    dataset = train_dataset[i]\n",
    "    image = dataset['image'].permute(1, 2, 0).numpy()\n",
    "    label = dataset['label']\n",
    "    class_label = dataset['class_label']\n",
    "    print(\"class_label = \", class_label)\n",
    "    angle = (45 * class_label - 180) / 180 * np.pi\n",
    "    vector_x = 150 * np.cos(angle)\n",
    "    vector_y = 150 * np.sin(angle)\n",
    "    plt.plot([label[0][0] + 100, label[0][0] + 100 + vector_x], [label[0][1] + 100, label[0][1] + 100 - vector_y], \"r-\")\n",
    "    plt.scatter(label[0][0] + 100 + vector_x, label[0][1] + 100 - vector_y, c=\"b\", s = 15)\n",
    "    print(\"angle is = \", np.arctan2(label[0][1] - label[1][1], label[1][0] - label[0][0]) * 180 / np.pi)\n",
    "    plt.imshow(image[:, :, 0:3])\n",
    "    plt.scatter(label[0][0], label[0][1], c = \"red\", s = 10)\n",
    "    plt.scatter(label[1][0], label[1][1], c = \"yellow\", s = 10)\n",
    "    plt.show()\n",
    "    plt.imshow(image[:, :, 3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(4100, 4103):\n",
    "    dataset = validation_dataset[i]\n",
    "    image = dataset['image'].permute(1, 2, 0).numpy()\n",
    "    label = dataset['label']\n",
    "    class_label = dataset['class_label']\n",
    "    print(\"class_label = \", class_label)\n",
    "    angle = (45 * class_label - 180) / 180 * np.pi\n",
    "    vector_x = 150 * np.cos(angle)\n",
    "    vector_y = 150 * np.sin(angle)\n",
    "    plt.plot([label[0][0] + 100, label[0][0] + 100 + vector_x], [label[0][1] + 100, label[0][1] + 100 - vector_y], \"r-\")\n",
    "    plt.scatter(label[0][0] + 100 + vector_x, label[0][1] + 100 - vector_y, c=\"b\", s = 15)\n",
    "    print(\"angle is = \", np.arctan2(label[0][1] - label[1][1], label[1][0] - label[0][0]) * 180 / np.pi)\n",
    "    plt.imshow(image[:, :, 0:3])\n",
    "    plt.scatter(label[0][0], label[0][1], c = \"red\", s = 10)\n",
    "    plt.scatter(label[1][0], label[1][1], c = \"yellow\", s = 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer=None, initial_learning_rate=1e-5, epoch=1, total_epochs=500):\n",
    "    \"\"\"\n",
    "    Adjust the learning rate based on the current epoch.\n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to adjust the learning rate for.\n",
    "        initial_learning_rate (int): Default is 1e-5.\n",
    "        epoch (int): The current epoch number.\n",
    "        total_epochs (int): Total epochs we want to train.\n",
    "    \"\"\"\n",
    "    # Define the learning rate schedule\n",
    "    initial_lr = initial_learning_rate\n",
    "    warmup_epochs = 5\n",
    "    cosine_decay_epochs = total_epochs\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        # Warm-up phase: Linearly increase the learning rate\n",
    "        lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        # Cosine annealing decay: Gradually reduce the learning rate\n",
    "        progress = (epoch - warmup_epochs) / cosine_decay_epochs\n",
    "        lr = initial_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    # Update the learning rate in the optimizer\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resnet18 network\n",
    "class Resnet_tool_tip_labelling_large(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  input size is 512 x 512 x 4\n",
    "  \"\"\"\n",
    "  def __init__(self, orig_resnet, n_classes=9):\n",
    "    super().__init__()\n",
    "    self.orig_resnet = orig_resnet\n",
    "    self.orig_resnet.avgpool = nn.AvgPool2d(kernel_size=(8,8), stride=(4,4))\n",
    "    self.orig_resnet.fc = nn.Linear(in_features=3*3*512, out_features=1000)\n",
    "    self.orig_resnet.out = nn.Linear(in_features=1000, out_features=n_classes)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # encoder\n",
    "    x1 = self.orig_resnet.conv1(x) # 64 x 256 x 256\n",
    "    x2 = self.orig_resnet.bn1(x1) \n",
    "    x3 = self.orig_resnet.relu(x2)\n",
    "    x4 = self.orig_resnet.maxpool(x3) # 64 x 128 x 128\n",
    "    x5 = self.orig_resnet.layer1(x4) # 64 x 128 x 128\n",
    "    x6 = self.orig_resnet.layer2(x5) # 128 x 64 x 64\n",
    "    x7 = self.orig_resnet.layer3(x6) # 256 x 32 x 32\n",
    "    x8 = self.orig_resnet.layer4(x7) # 512 x 16 x 16\n",
    "    x9 = self.orig_resnet.avgpool(x8) # 512 x 3 x 3\n",
    "    x10 = x9.reshape(x9.shape[0], -1) # N x (512 x 3 x 3)\n",
    "    x11 = self.orig_resnet.relu(self.orig_resnet.fc(x10)) # N x 1000\n",
    "    x12 = self.orig_resnet.out(x11) # N x 9\n",
    "    return x12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model\n",
    "print(\"USING THE FIRST GPU\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"USING DEVICE\", device)\n",
    "\n",
    "# use volumetric output with skip-connections and more decoders\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "n_input_channel = 4\n",
    "resnet.conv1 = torch.nn.Conv2d(n_input_channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model_ft = Resnet_tool_tip_labelling_large(resnet) \n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((512, 512, 4))\n",
    "image = TF.to_tensor(image)\n",
    "image = image.unsqueeze(0).float()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_ft(image)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training function\n",
    "def train_model(model, criterion, optimizer, scheduler=None, num_epochs=25, initial_lr=1e-5, device=None):\n",
    "    since = time.time()\n",
    "    \n",
    "    # initialization every epoch\n",
    "    best_validation_model_output = None\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_loss = 1000000\n",
    "\n",
    "    best_acc = -1000\n",
    "\n",
    "    train_losses = [] \n",
    "    test_losses = []\n",
    "    final_accuracies = []\n",
    "    \n",
    "    running_loss_avg = None\n",
    "    all_info = []\n",
    "    # no need to resample test loader every epoch\n",
    "    val_dataloader = DataLoader(validation_dataset, batch_size = 128, shuffle = False)\n",
    "    for epoch in range(num_epochs):\n",
    "        adjust_learning_rate(optimizer, initial_lr, epoch, num_epochs)\n",
    "        # sample from dataloader every epoch\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True)\n",
    "        dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        error_per_sample_for_epoch = 0.0\n",
    "        for phase in ['train', 'val']:\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            running_loss_avg = 0.0\n",
    "            absolute_error_list = []\n",
    "            absolute_error_list_x = []\n",
    "            absolute_error_list_y = []\n",
    "            absolute_error_list_z = []\n",
    "            \n",
    "            model_outputs = []\n",
    "            \n",
    "            if phase == 'train':\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode   \n",
    "\n",
    "            # Iterate over data.\n",
    "            progress_bar = tqdm.tqdm(dataloaders[phase], desc=phase)\n",
    "            for batch_idx, data in enumerate(progress_bar):\n",
    "                inputs = data['image'].to(device).float()\n",
    "                labels = data['label'].to(device)\n",
    "                class_labels = data['class_label'].to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, class_labels)\n",
    "                    # calculate error\n",
    "                    output_np = output.cpu().detach().numpy()\n",
    "                    for i in range(len(output_np)):\n",
    "                        # get current output\n",
    "                        a = output_np[i,:]\n",
    "                        # find the argmax and reshape into xy coords\n",
    "                        current_argmax = a.argmax()\n",
    "                        # get the correct xy coords\n",
    "                        cur_class_label = class_labels[i].cpu().detach().numpy()\n",
    "                        error = np.absolute(current_argmax - cur_class_label)\n",
    "                        if (cur_class_label == 8 and current_argmax != 8):\n",
    "                            error = 4\n",
    "                        elif (error > 4):\n",
    "                            error = 8 - error\n",
    "                        \n",
    "                        absolute_error_list.append(error)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Loss\n",
    "                running_loss += loss.item() \n",
    "                running_loss_avg = running_loss / (batch_idx + 1)\n",
    "                running_error = np.sum(absolute_error_list) / len(absolute_error_list)\n",
    "\n",
    "                progress_bar.set_description(\n",
    "                'loss avg by batch: {:.4}, running_abs_error: {:.2f}'.format(\n",
    "                    running_loss / (batch_idx + 1), running_error) )\n",
    "            \n",
    "            # record train/test loss\n",
    "            if phase == 'train':\n",
    "                train_losses.append(running_loss)\n",
    "                all_info.append([epoch, \"train\", running_error])\n",
    "                \n",
    "            if phase == 'val':\n",
    "                test_losses.append(running_loss)\n",
    "                all_info.append([epoch, \"val\", running_error])\n",
    "            \n",
    "        # per epoch loop\n",
    "        # save model_parameters every epoch\n",
    "        if not os.path.exists(\"./train_parameters\"):\n",
    "            os.makedirs(\"./train_parameters\")\n",
    "        save_checkpoint({'state_dict': model.state_dict(), 'optimizer':optimizer.state_dict()},\n",
    "                       \"./training_parameters/epoch_\" + str(epoch) + phase + \"_checkpoint.pth.tar\")\n",
    "\n",
    "        np.savetxt(\"progress_log.csv\", np.asarray(all_info), delimiter=\",\", fmt=\"%s\")\n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_validation_model_output, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import cv2\n",
    "import random \n",
    "# setup model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "n_input_channel = 4\n",
    "resnet.conv1 = torch.nn.Conv2d(n_input_channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# choose resnet of your choice\n",
    "resnet_enc_dec_concat_more = Resnet_tool_tip_labelling_large(resnet)\n",
    "# resnet_enc_dec_concat_more = nn.DataParallel(resnet_enc_dec_concat_more)\n",
    "model_ft = resnet_enc_dec_concat_more.to(device)\n",
    "# choose criteria\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion2 = nn.MSELoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.003)\n",
    "# LODA OPTIMIZER PARAM\n",
    "#optimizer_ft.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# model_ft, best_validation_model_output, train_losses, test_losses, predicted_imgs = train_model(\n",
    "#             model = model_ft,\n",
    "#             criterion1 = criterion1, criterion2 = criterion2,\n",
    "#             optimizer = optimizer_ft,\n",
    "#             num_epochs= 50, \n",
    "#             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# camera image training - 27000 images - aggressive augmentation - 9 classes - lr 0.003 best 178\n",
    "model_ft, best_validation_model_output, train_losses, test_losses = train_model(\n",
    "            model = model_ft,\n",
    "            criterion = criterion,\n",
    "            optimizer = optimizer_ft,\n",
    "            num_epochs = 200,\n",
    "            initial_lr = 0.003,\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
